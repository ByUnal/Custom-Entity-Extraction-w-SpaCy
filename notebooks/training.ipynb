{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb31446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\m84246307\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import ast\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import DocBin\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243f22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"data/jobpostings.csv\")\n",
    "\n",
    "# Remove all duplicate rows \n",
    "df = df.drop_duplicates(keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819b98c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Job Id', 'Job Title', 'SOC Code', 'Job Description', 'Company Name',\n",
       "       'Skills', 'Qualification', 'City', 'State', 'Zipcode',\n",
       "       'Job Opening Date', 'Job Closing Date', 'Status', 'Website Url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95facd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Local Media, Editing, Journalism]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Skills\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db1f30",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d910d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stopwords])\n",
    "\n",
    "def preprocessing(description):\n",
    "    \n",
    "    # remove punctuation\n",
    "    description = remove_punctuation(str(description))\n",
    "                                                            \n",
    "    # lowering the text\n",
    "    description = description.lower()\n",
    "    \n",
    "    # remove stopwords\n",
    "    description = remove_stopwords(description)\n",
    "    \n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd423e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188627it [06:22, 493.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing...\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    df['Job Description'][idx] = preprocessing(df['Job Description'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc5b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tech = pd.read_excel(\"data/Technology Skills.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31229585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31078, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tech.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47177044",
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_skills = list(df_tech[\"Example\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8790712f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8869"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tech_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57491d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adobe Systems Adobe Acrobat',\n",
       " 'AdSense Tracker',\n",
       " 'Atlassian JIRA',\n",
       " \"Blackbaud The Raiser's Edge\",\n",
       " 'ComputerEase Construction Accounting',\n",
       " 'Database reporting software',\n",
       " 'Databox',\n",
       " 'Email software',\n",
       " 'Enterprise resource planning ERP software',\n",
       " 'Exact Software Macola ES Labor Performance']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_skills[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da21a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skill = pd.read_excel(\"data/Skills.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc49a86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['O*NET-SOC Code', 'Title', 'Element ID', 'Element Name', 'Scale ID',\n",
       "       'Scale Name', 'Data Value', 'N', 'Standard Error', 'Lower CI Bound',\n",
       "       'Upper CI Bound', 'Recommend Suppress', 'Not Relevant', 'Date',\n",
       "       'Domain Source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_skill.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db25161f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61110, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_skill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3287609",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_skills = list(df_skill[\"Element Name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d1eb3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reading Comprehension',\n",
       " 'Active Listening',\n",
       " 'Writing',\n",
       " 'Speaking',\n",
       " 'Mathematics',\n",
       " 'Science',\n",
       " 'Critical Thinking',\n",
       " 'Active Learning',\n",
       " 'Learning Strategies',\n",
       " 'Monitoring']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_skills[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6336fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(regular_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de8cc1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = tech_skills + regular_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09953ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skills should be unique\n",
    "skills = list(set(skills))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6777d70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8904"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab559da4",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cce232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary will contain all annotated examples\n",
    "collective_dict = {'TRAINING_DATA': []}\n",
    "\n",
    "def structure_training_data(text, kw_list):\n",
    "    results = []\n",
    "    entities = []\n",
    "    \n",
    "    # search for instances of keywords within the text (ignoring letter case)\n",
    "    for kw in kw_list:\n",
    "        \n",
    "        # Check whether kw equals to \"R\", \"J\", ...\n",
    "        try:\n",
    "            search = re.finditer(f\" {kw} \", text, flags=re.IGNORECASE)\n",
    "        except:\n",
    "            # To avoid \"multiple repeat\" error for inputs have special chars. like \"c++\"\", we use re.escape(). \n",
    "            search = re.finditer(re.escape(f\" {kw} \"), text, flags=re.IGNORECASE)\n",
    "              \n",
    "        # store the start/end character positions\n",
    "        all_instances = [[m.start(),m.end()] for m in search] \n",
    "        \n",
    "        # if the callable_iterator found matches, create an 'entities' list\n",
    "        if len(all_instances)>0:\n",
    "            for i in all_instances:\n",
    "                start = i[0]\n",
    "                end = i[1]\n",
    "                entities.append((start, end, kw))\n",
    "            \n",
    "        # alert when no matches are found given the user inputs\n",
    "#         else:\n",
    "#             print(\"No pattern matches found. Keyword:\", kw)\n",
    "                \n",
    "    # add any found entities into a JSON format within collective_dict\n",
    "    if len(entities)>0:\n",
    "        results = [text, {\"entities\": entities}]\n",
    "        collective_dict['TRAINING_DATA'].append(results)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b3a073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=14, shuffle=True)\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e63813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150901, 15)\n",
      "(37726, 15)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60b47bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "\n",
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "def prepare_training_data(row):\n",
    "    # Convert string to list\n",
    "    # Clear the whitespaces at the beginning and the end of each item\n",
    "    annotated_skills = convert_to_list(row['Skills'])\n",
    "    if annotated_skills:\n",
    "        annotated_skills = [s.strip() for s in annotated_skills]\n",
    "\n",
    "        # Merge skills list with already annotated skills\n",
    "        merged_skills = skills + annotated_skills\n",
    "    \n",
    "        # shape the training data\n",
    "        structure_training_data(row['Job Description'], merged_skills)\n",
    "    else:\n",
    "        # shape the training data\n",
    "        structure_training_data(row['Job Description'], skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6f39075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(string_shape_list):\n",
    "    try:\n",
    "        return string_shape_list.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "    except AttributeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afb8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847it [30:25,  1.06it/s]"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(train.iterrows()):\n",
    "    prepare_training_data(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be2aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46057it [10:12:41,  1.06it/s]                                                                                          "
     ]
    }
   ],
   "source": [
    "# define our training data to TRAIN_DATA\n",
    "TRAIN_DATA = collective_dict['TRAINING_DATA']\n",
    "\n",
    "# create a blank model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "def create_training_set(TRAIN_DATA):\n",
    "    db = DocBin()\n",
    "    for text, annot in tqdm(TRAIN_DATA):\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "\n",
    "        # create span objects\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            \n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "\n",
    "            # skip if the character indices do not map to a valid span\n",
    "            if span is None:\n",
    "                # print(\"Skipping entity.\")\n",
    "                continue\n",
    "            else:\n",
    "                ents.append(span)\n",
    "                # handle erroneous entity annotations by removing them\n",
    "                try:\n",
    "                    doc.ents = ents\n",
    "                except:\n",
    "                    # print(\"BAD SPAN:\", span, \"\\n\")\n",
    "                    ents.pop()\n",
    "        doc.ents = ents\n",
    "    \n",
    "        # pack Doc objects into DocBin\n",
    "        db.add(doc)\n",
    "    return db\n",
    "\n",
    "TRAIN_DATA_DOC = create_training_set(TRAIN_DATA)\n",
    "\n",
    "# Export results (here I add it to a TRAIN_DATA folder within the directory)\n",
    "TRAIN_DATA_DOC.to_disk(\"./TRAIN_DATA/TRAIN_DATA.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text, annot in tqdm(TRAIN_DATA):\n",
    "#     doc = nlp.make_doc(text)\n",
    "#     span = doc.char_span(272, 277, label='SKILL')\n",
    "#     print(span)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4400d",
   "metadata": {},
   "source": [
    "# Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = test[\"Job Description\"][9]\n",
    "# load the trained model\n",
    "nlp_output = spacy.load(\"output/model-best\")\n",
    "\n",
    "# pass our test instance into the trained pipeline\n",
    "doc = nlp_output(model_test)\n",
    "\n",
    "# # customize the label colors\n",
    "# colors = {\"SERVICE\": \"linear-gradient(90deg, #E1D436, #F59710)\"}\n",
    "# options = {\"ents\": [\"SERVICE\"], \"colors\": colors}\n",
    "\n",
    "# # visualize the identified entities\n",
    "# displacy.render(doc, style=\"ent\", options=options)\n",
    "\n",
    "# print out the identified entities\n",
    "{\"Job Id\": test[\"Job Id\"][0],\"Entity Values\": list(doc.ents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ed616",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Job Description\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3fb82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
